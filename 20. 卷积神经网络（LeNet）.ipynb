{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 卷积神经网络就是含卷积层的网络\n",
    "# LeNet\n",
    "# 卷积层块里的基本单位是卷积层后接最大池化层:卷积层用来识别图像里的空间模式,\n",
    "# 例如线条和物体局部,之后的最大池化层则用来降低卷积层对位置的敏感性。\n",
    "import gluonbook as gb\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(\n",
    "    nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "    nn.MaxPool2D(pool_size=2, strides=2),\n",
    "    nn.Dense(120, activation='sigmoid'),\n",
    "    nn.Dense(84, activation='sigmoid'),\n",
    "    nn.Dense(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 6, 24, 24)\n",
      "pool0 output shape:\t (1, 6, 12, 12)\n",
      "conv1 output shape:\t (1, 16, 8, 8)\n",
      "pool1 output shape:\t (1, 16, 4, 4)\n",
      "dense0 output shape:\t (1, 120)\n",
      "dense1 output shape:\t (1, 84)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# (h - k + 1 + p + s) / s向下取整\n",
    "X = nd.random.uniform(shape=(1, 1, 28, 28))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取数据和训练\n",
    "batch_size = 256\n",
    "train_iter, test_iter = gb.load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_gpu4():\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.zeros((1,), ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = try_gpu4()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, ctx):\n",
    "    acc = nd.array([0], ctx=ctx)\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "        acc += gb.accuracy(net(X), y)\n",
    "    return acc.asscalar() / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
    "             num_epochs):\n",
    "    print('training on', ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, start = 0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += l.mean().asscalar()\n",
    "            train_acc_sum += gb.accuracy(y_hat, y)\n",
    "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
    "        print(('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "                'time %.1f sec' % (epoch + 1, train_l_sum / len(train_iter),\n",
    "                train_acc_sum / len(train_iter),\n",
    "                test_acc, time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 2.3190, train acc 0.100, test acc 0.098, time 4.9 sec\n",
      "epoch 2, loss 1.9952, train acc 0.235, test acc 0.534, time 3.7 sec\n",
      "epoch 3, loss 0.9875, train acc 0.609, test acc 0.700, time 3.3 sec\n",
      "epoch 4, loss 0.7596, train acc 0.705, test acc 0.732, time 3.9 sec\n",
      "epoch 5, loss 0.6741, train acc 0.734, test acc 0.758, time 3.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 5\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
